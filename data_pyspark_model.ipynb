{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c39a8a-3d17-459d-a154-242edd99213d",
   "metadata": {},
   "source": [
    "***\n",
    "# Robust Journey Planning\n",
    "\n",
    "**Link to project presentation:** https://youtube.com\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95dd12-c010-4675-b7dd-718466209880",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [0. Imports](#imports)\n",
    "    * [0.1 HDFS/Hive](#hive)\n",
    "    * [0.2 Spark](#spark)\n",
    "    * [0.3 Geospatial User Defined Functions](#udf)\n",
    "* [1. Data](#data)\n",
    "    * [1.1 Timetable](#timetablegeostops)\n",
    "    * [1.2 Actual Data](#actualdata)\n",
    "    * [1.3 Geo Shapes](#geoshapes)\n",
    "    * [1.4 Weather Data](#weather)\n",
    "* [2. Data Preprocessing](#datapreprocessing)\n",
    "    * [2.1 Preprocessing Timetable & Geostops](#preprocessingtimetablegeostops)\n",
    "    * [2.2 Preprocessing Istdaten Data](#preprocessingactualdata)\n",
    "* [3. Building the Transportation Graph](#transportationgraph)\n",
    "    * [3.1 test](#test)\n",
    "* [4. Modelling Delays](#modellingdelays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d75e0-92f3-4242-b5ec-2cf0bbbd03ff",
   "metadata": {},
   "source": [
    "## 0. Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "In this section we import necessary packages, connect to HDFS / Hive and initialize the Spark environment we will use in the assignment. Finally, we will add support for Geospatial User Defined Funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8aef980-73f6-41d4-8271-2135e5ba769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4336</td><td>application_1713270977862_4678</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster080.iccluster.epfl.ch:8088/proxy/application_1713270977862_4678/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster077.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1713270977862_4678_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INSERT A REGION OBJECTID\n",
    "OBJECTID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "548cfefa-c411-427e-8791-39c74a17f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cluster-based imports\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql import SparkSession, Row, HiveContext, Window,  functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, StructField, StructType\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc97714-dc6f-4083-b476-ffeee3b6290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# Local imports for visualization and data manipulation\n",
    "from pyarrow.fs import HadoopFileSystem, FileSelector\n",
    "from pyhive import hive\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb1c7e7-5ddd-43aa-be87-2f75372eebef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model part \n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcd68a-add5-4e8c-a2d2-9c0059abf4aa",
   "metadata": {},
   "source": [
    "### 0.1 HDFS/Hive <a class=\"anchor\" id=\"hive\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c25515-0961-4f96-a789-887363986e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.fs.FileSystem).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop HDFS URL: hdfs://iccluster067.iccluster.epfl.ch:8020\n",
      "Username: schiffer\n",
      "Connected to Hive at: iccluster080.iccluster.epfl.ch:10000\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Environment variables setup\n",
    "default_db = 'com490'\n",
    "hive_server = os.environ.get('HIVE_SERVER', 'iccluster080.iccluster.epfl.ch:10000')\n",
    "hadoop_fs = os.environ.get('HADOOP_DEFAULT_FS', 'hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "hdfs = HadoopFileSystem.from_uri(hadoop_fs)\n",
    "username = os.environ.get('USER', 'anonym')\n",
    "hive_host, hive_port = hive_server.split(':')\n",
    "\n",
    "# Connect to Hive\n",
    "conn = hive.connect(host=hive_host, port=int(hive_port), username=username)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Print connection details\n",
    "print(f\"Hadoop HDFS URL: {hadoop_fs}\")\n",
    "print(f\"Username: {username}\")\n",
    "print(f\"Connected to Hive at: {hive_host}:{hive_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abec0a4d-acdc-4383-ba71-d3a6c0ad4c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /user/schiffer/finalproject\n"
     ]
    }
   ],
   "source": [
    "%%local \n",
    "\n",
    "# Create directory for finalproject if it doesn't already exist\n",
    "base_path = f\"/user/{username}/\"\n",
    "\n",
    "# List all files and directories in the base path\n",
    "selector = FileSelector(base_path, recursive=False)\n",
    "file_info_list = hdfs.get_file_info(selector)\n",
    "\n",
    "# Directory to check\n",
    "target_directory = \"finalproject\"\n",
    "\n",
    "# Check if the target directory exists among the listed files/directories\n",
    "directory_exists = any(info.path.rstrip('/').split('/')[-1] == target_directory for info in file_info_list)\n",
    "\n",
    "if not directory_exists:\n",
    "    # Create the directory if it does not exist\n",
    "    directory_path = f\"{base_path}{target_directory}\"\n",
    "    hdfs.create_dir(directory_path, recursive=True)\n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {base_path}{target_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70fc4c5a-18b2-40d8-9bb7-4054d540d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database schiffer created or already exists.\n",
      "Switched to database: schiffer\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Create a new database\n",
    "query = f\"CREATE DATABASE IF NOT EXISTS {username} LOCATION '/user/{username}/finalproject'\"\n",
    "cur.execute(query)\n",
    "print(f\"Database {username} created or already exists.\")\n",
    "\n",
    "# Switch to the new database\n",
    "query = f\"USE {username}\"\n",
    "cur.execute(query)\n",
    "print(f\"Switched to database: {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78c868f8-8f0f-4e39-8df1-b88c5433244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geo_shapes',),\n",
       " ('sbb_lausanne_trip_times',),\n",
       " ('sbb_orc_istdaten',),\n",
       " ('sbb_orc_stops',),\n",
       " ('sbb_stop_times_lausanne_region',),\n",
       " ('sbb_stop_to_stop_lausanne_region',),\n",
       " ('sbb_stops_lausanne',),\n",
       " ('sbb_stops_lausanne_region',)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "cur.execute(f\"SHOW TABLES IN {username}\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219d8547-4372-4024-b635-480baccf424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Make sure to give rw access to Hive and Livy\n",
    "!hdfs dfs -setfacl -R -m user:hive:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m default:user:hive:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m user:livy:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m default:user:livy:rwx /user/${USER}/finalproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c88fca14-756d-4e6b-8bee-4c5747409e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'livy'"
     ]
    }
   ],
   "source": [
    "# Remember, when not using %%local our username is 'livy'\n",
    "local_username = os.environ.get('USER', getpass.getuser())\n",
    "local_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "deda4f85-54b4-4100-8fcd-71d809251f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'schiffer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef3203-7a82-49de-b39c-4f5dce73d777",
   "metadata": {},
   "source": [
    "### 0.2 Spark <a class=\"anchor\" id=\"Spark\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1768d20-9052-4d20-a324-440fbc7e712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fcdc09ba220>\n",
      "<class 'pyspark.sql.session.SparkSession'>"
     ]
    }
   ],
   "source": [
    "# Initializing the spark session and sending %%local {username} to Spark\n",
    "sparkSession = SparkSession.builder.appName('final-project-{0}'.format(getpass.getuser())).getOrCreate()\n",
    "sparkSession.getActiveSession()\n",
    "print(sparkSession.getActiveSession())\n",
    "print(type(sparkSession))\n",
    "sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f9046f-6778-4aaa-9b75-cbf8e312b284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711bb814-d6cf-49f8-89f0-084a3d703743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| schiffer|       sbb_orc_stops|      false|\n",
      "| schiffer|  sbb_stops_lausanne|      false|\n",
      "| schiffer|    sbb_orc_istdaten|      false|\n",
      "| schiffer|sbb_stops_lausann...|      false|\n",
      "| schiffer|sbb_stop_to_stop_...|      false|\n",
      "| schiffer|sbb_stop_times_la...|      false|\n",
      "| schiffer|sbb_lausanne_trip...|      false|\n",
      "| schiffer|          geo_shapes|      false|\n",
      "+---------+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "# Check that Spark has access to personal HDFS\n",
    "spark.sql(f\"SHOW TABLES IN {username}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4054b714-36bc-41e2-8a6b-67fb4b90fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'hadoop_fs' as 'hadoop_fs' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i hadoop_fs -t str -n hadoop_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb18555-1747-41a9-b59c-0a8f3e69d036",
   "metadata": {},
   "source": [
    "### 0.3 Geospatial User Defined Functions <a class=\"anchor\" id=\"udf\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a3cc4-a636-47d5-bdcf-bbc32a25a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4615484929634ab686e9f9053eac50b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "ADD JARS\n",
    "    {hadoop_fs}/data/jars/esri-geometry-api-2.2.4.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-hive-2.2.0.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-json-2.2.0.jar\n",
    "\"\"\")\n",
    "\n",
    "# Create or replace temporary functions\n",
    "gis_functions = [\n",
    "    \"ST_Point\", \"ST_Distance\", \"ST_SetSRID\", \"ST_GeodesicLengthWGS84\",\n",
    "    \"ST_LineString\", \"ST_AsBinary\", \"ST_PointFromWKB\", \"ST_GeomFromWKB\", \"ST_Contains\"\n",
    "]\n",
    "\n",
    "for func in gis_functions:\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMPORARY FUNCTION {func} AS 'com.esri.hadoop.hive.{func}'\")\n",
    "\n",
    "# Get the list of functions\n",
    "functions_df = spark.sql(\"SHOW FUNCTIONS\")\n",
    "\n",
    "# Filter and show only functions starting with 'st_'\n",
    "functions_df.filter(F.col(\"function\").startswith(\"st_\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4ef4f-1f72-458b-a6b0-8853b20f1ad7",
   "metadata": {},
   "source": [
    "## 1. Data <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "In this section, we will load and preprocess first the timetable and geostops data and finally the Istdaten data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117417ff-2f13-4e5e-ab69-0fe760187b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to check that everything looks normal\n",
    "print(f\"remote USER={os.getenv('USER',None)}\")\n",
    "print(f\"local USER={username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49be96b8-82f8-46ed-a094-fb4544f14621",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "print(f\"local USER={os.getenv('USER',None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3660f7d-4d0c-4e7c-a515-70a72d3ac70b",
   "metadata": {},
   "source": [
    "### 1.1 Timetables<a class=\"anchor\" id=\"timetables\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580414e-ca75-46e6-ad73-8a753ccf5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load timetables data from HDFS\n",
    "stops = spark.read.orc('/data/sbb/orc/timetables/stops/year=2024/month=5/day=16')\n",
    "stop_times = spark.read.orc('/data/sbb/orc/timetables/stop_times/year=2024/month=5/day=16')\n",
    "trips = spark.read.orc('/data/sbb/orc/timetables/trips/year=2024/month=5/day=16')\n",
    "calendar = spark.read.orc('/data/sbb/orc/timetables/calendar/year=2024/month=5/day=16')\n",
    "routes = spark.read.orc('/data/sbb/orc/timetables/routes/year=2024/month=5/day=16')\n",
    "transfers = spark.read.orc('/data/sbb/orc/timetables/transfers/year=2024/month=5/day=16')\n",
    "\n",
    "stops.show(2)\n",
    "stop_times.show(2)\n",
    "trips.show(2)\n",
    "calendar.show(2)\n",
    "routes.show(2)\n",
    "transfers.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1cb7b-60b4-44a7-b5db-e4e634d27e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfers.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa5293-9960-4ca6-8869-070f54eae78e",
   "metadata": {},
   "source": [
    "### 1.2 Istdaten <a class=\"anchor\" id=\"istdaten\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70e7b4b-60db-4c67-902d-a15224a807b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "istdaten = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "\n",
    "#istdaten.printSchema()\n",
    "istdaten.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751a612-4a17-4f29-b17d-16c238cd7c35",
   "metadata": {},
   "source": [
    "### 1.3 Geo Shapes <a class=\"anchor\" id=\"geoshapes\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4dabf-5fac-499a-8acb-71271c549722",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {username}.geo_shapes\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE {username}.geo_shapes(\n",
    "    objectid INT,\n",
    "    name     STRING,\n",
    "    geometry BINARY\n",
    ")\n",
    "PARTITIONED BY(country STRING, region STRING)\n",
    "ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.GeoJsonSerDe' \n",
    "STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedEsriJsonInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '/data/geo/json/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0adc38f-34ef-489e-9fc5-3044b8d9c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"MSCK REPAIR TABLE {username}.geo_shapes\")\n",
    "spark.sql(f\"SELECT * FROM {username}.geo_shapes ORDER BY objectid ASC\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d47933-ba27-4799-97f6-dd9545795be7",
   "metadata": {},
   "source": [
    "### 1.4 Optional: Weather Data <a class=\"anchor\" id=\"weatherdata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0083b-23cb-43ff-a480-ccddb793fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add code from hw3\n",
    "stations = spark.read.csv('/data/wunderground/csv/stations', header=True)\n",
    "weather_data = spark.read.json('/data/wunderground/json/history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca34514-08bb-4b2b-8a1a-2189d616855c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799d8615-f567-40bc-801a-7b09400a2d10",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing <a class=\"anchor\" id=\"datapreprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1351c1e-99f6-434c-81d9-3a8fc192dce7",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Timetable & Geostops <a class=\"anchor\" id=\"preprocessingtimetablegeostops\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d6284b-8849-432b-8896-ec49d02f52ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stops that are not in the specified object_id\n",
    "object_id = 2 \n",
    "\n",
    "stops_region = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    a.stop_id,\n",
    "    a.stop_lat,\n",
    "    a.stop_lon\n",
    "FROM {username}.sbb_orc_stops a JOIN {username}.geo_shapes b\n",
    "ON ST_Contains(b.geometry, ST_Point(a.stop_lon, a.stop_lat))\n",
    "WHERE b.objectid = {object_id}\n",
    "\"\"\")\n",
    "\n",
    "stops_region.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9af2b0-4972-4433-af2b-a85f8af309fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the stops_region dataframe as a temporary view to use in SQL\n",
    "stops_region.createOrReplaceTempView(\"stops_region\")\n",
    "\n",
    "# Execute the query\n",
    "stop_pairs_within_500m = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    a.stop_id AS stop_id1,\n",
    "    b.stop_id AS stop_id2,\n",
    "    a.stop_lat AS stop_lat1,\n",
    "    a.stop_lon AS stop_lon1,\n",
    "    b.stop_lat AS stop_lat2,\n",
    "    b.stop_lon AS stop_lon2,\n",
    "    ST_GeodesicLengthWGS84(ST_SetSRID(ST_LineString(a.stop_lon, a.stop_lat, b.stop_lon, b.stop_lat), 4326)) AS distance_meters\n",
    "FROM \n",
    "    stops_region a \n",
    "CROSS JOIN \n",
    "    stops_region b\n",
    "WHERE \n",
    "    a.stop_id != b.stop_id\n",
    "    AND ST_GeodesicLengthWGS84(ST_SetSRID(ST_LineString(array(ST_Point(a.stop_lon, a.stop_lat), ST_Point(b.stop_lon, b.stop_lat))), 4326)) < 500\n",
    "\"\"\")\n",
    "\n",
    "# Calculate walking transfer times in minutes\n",
    "# We assume that 2min mininum are required for transfers within a same location, \n",
    "# to which we add 1min per 50m walking time to connect two stops at most 500m appart\n",
    "stop_pairs_within_500m = stop_pairs_within_500m.withColumn(\n",
    "    \"transfer_time_minutes\",\n",
    "    F.expr(\"2 + (distance_meters / 50)\")\n",
    ")\n",
    "# Show results\n",
    "stop_pairs_within_500m.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a15fb-9e46-4f34-a0f9-5cab60e9d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to focus only on the weekday schedule\n",
    "calendar.show(2)\n",
    "print(f\"Trips full week: {calendar.count()}\")\n",
    "\n",
    "# Filter for services that are active from Monday to Friday\n",
    "weekday_calendar = calendar.filter(\n",
    "    (F.col(\"monday\") == True) &\n",
    "    (F.col(\"tuesday\") == True) &\n",
    "    (F.col(\"wednesday\") == True) &\n",
    "    (F.col(\"thursday\") == True) &\n",
    "    (F.col(\"friday\") == True)\n",
    ").select(\"service_id\").distinct()\n",
    "\n",
    "print(f\"Trips weekday: {weekday_calendar.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2042304f-0b8b-43a6-b023-82c1e2e4586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join trips with weekday calendar\n",
    "weekday_calendar_trips = trips.join(weekday_calendar, \"service_id\")\n",
    "\n",
    "# Add transportation type info (bus, train etc.) from the routes table\n",
    "weekday_calendar_trips = weekday_calendar_trips.join(routes.select(\"route_id\", \"route_desc\"), \"route_id\")\n",
    "\n",
    "# Use trip_id as a key to join stop_times with weekday_trips.\n",
    "weekday_calendar_stop_times = stop_times.join(weekday_calendar_trips, \"trip_id\")\n",
    "\n",
    "# Join with stops_region to focus only on stops within the specific region\n",
    "weekday_all_info = weekday_calendar_stop_times.join(stops_region, \"stop_id\")\n",
    "columns_to_drop = ['pickup_type', 'drop_off_type']\n",
    "weekday_all_info = weekday_all_info.drop(*columns_to_drop)\n",
    "weekday_all_info.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484abbdc-785b-4e58-a510-de282d9a25a0",
   "metadata": {},
   "source": [
    "1. Connections Table:\n",
    "- This table should capture all possible connections between stops. It should include fields for departure stop, arrival stop, departure time, arrival time, and transport mode (bus, train, etc.), taking into account both direct transit connections and possible walking transfers.\n",
    "- For walking transfers, you should consider all stops within 500 meters of each other as potentially connected. Calculate walking times based on the distance and a fixed walking speed (50m/1min).\n",
    "\n",
    "2. Journey Table:\n",
    "- Create a \"journeys\" table that aggregates data from stop_times, trips, routes, and calendar. It should include trip information (route, departure times, arrival times, etc.) and filter for typical business days using the calendar table.\n",
    "- Incorporate service variations and trip cancellations to ensure accuracy.\n",
    "\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b959309-b5d3-41cc-b916-726ccec2ac53",
   "metadata": {},
   "source": [
    "1. **Connections Table**:\n",
    "- This table should capture all possible connections between stops. It should include fields for departure stop, arrival stop, departure time, arrival time, and transport mode (bus, train, etc.), taking into account both direct transit connections and possible walking transfers.\n",
    "- For walking transfers, you should consider all stops within 500 meters of each other as potentially connected. Calculate walking times based on the distance and a fixed walking speed (50m/1min).\n",
    "\n",
    "2. **Journey Table**:\n",
    "- Create a \"journeys\" table that aggregates data from stop_times, trips, routes, and calendar. It should include trip information (route, departure times, arrival times, etc.) and filter for typical business days using the calendar table.\n",
    "- Incorporate service variations and trip cancellations to ensure accuracy.\n",
    "\n",
    "3. **Transfers Table**:\n",
    "- This table should specifically account for possible transfers between routes, considering both transit and walking transfers.\n",
    "- You may include an estimated transfer time (including buffer times for realistic transfers between different modes of transport).\n",
    "\n",
    "4. **Routes and Confidence Table**:\n",
    "- This is a critical table that will compute potential routes from stop A to stop B considering your criteria (e.g., arrival before a specific time T with a confidence level Q).\n",
    "- Use statistical methods or historical data to assess the reliability of each route segment to establish confidence levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7da425-e2f8-4a4d-957a-3836e5ac2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connections table\n",
    "# Register the DataFrames as temporary views\n",
    "stops.createOrReplaceTempView(\"stops\")\n",
    "stop_times.createOrReplaceTempView(\"stop_times\")\n",
    "trips.createOrReplaceTempView(\"trips\")\n",
    "routes.createOrReplaceTempView(\"routes\")\n",
    "calendar.createOrReplaceTempView(\"calendar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9eab9f-cd75-4637-9759-0ddcc422e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query and create a DataFrame\n",
    "transit_connections = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    st1.stop_id AS departure_stop_id,\n",
    "    st2.stop_id AS arrival_stop_id,\n",
    "    st1.departure_time,\n",
    "    st2.arrival_time,\n",
    "    rt.route_desc AS transport_mode\n",
    "FROM \n",
    "    stop_times st1\n",
    "JOIN \n",
    "    stop_times st2 ON st1.trip_id = st2.trip_id AND st1.stop_sequence = st2.stop_sequence - 1\n",
    "JOIN \n",
    "    trips tr ON st1.trip_id = tr.trip_id\n",
    "JOIN \n",
    "    routes rt ON tr.route_id = rt.route_id\n",
    "\"\"\")\n",
    "transit_connections.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdc9859-3d9e-4f9d-98b3-943ab6f03c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1fdf9d-2795-4e0a-8d2a-70996a2eb4d7",
   "metadata": {},
   "source": [
    "### 2.2 Preprocessing Istdaten Data <a class=\"anchor\" id=\"preprocessingactualdata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f088954-b5a5-4f4d-84c8-d4728b635aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb8123-3ecf-4f24-b417-42867df8e881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b1c620ae-f2f9-4419-a8f4-a8071081457d",
   "metadata": {},
   "source": [
    "## 3. Building the Transportation Graph <a class=\"anchor\" id=\"transportationgraph\"></a>\n",
    "Simplifying assumptions:\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a669463-0405-46d4-9448-38f0b11c54c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b06e83f-1583-41d2-979c-63374ffbb5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42680931-a19b-4713-87d8-6f156ea424a9",
   "metadata": {},
   "source": [
    "## 4. Modelling Delays <a class=\"anchor\" id=\"modellingdelays\"></a>\n",
    "Simplifying assumptions:\n",
    "1. **Delay Label Creation** : We assume that any delay greater than 30 minutes should not be taken into account. For delays less than 30 minutes, the delay is rounded up to the nearest whole minute using F.ceil, and the resulting value is cast to an integer. The primary assumption here is that delays of 30 minutes are quite rare and there would always be another connection that would enable to take a more efficient route "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b21ca-a830-4d1f-ad24-cf61339778ca",
   "metadata": {},
   "source": [
    "#### On Mock Data to test it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda9810-a49b-4d69-8a3c-6137f1adea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define schema for mock data\n",
    "schema = StructType([\n",
    "    StructField(\"bpuic\", IntegerType(), True),\n",
    "    StructField(\"ankunftszeit\", StringType(), True),\n",
    "    StructField(\"linien_id\", IntegerType(), True),\n",
    "    StructField(\"avg_delay\", DoubleType(), True),\n",
    "    StructField(\"stddev_delay\", DoubleType(), True),\n",
    "    StructField(\"temperature\", DoubleType(), True),\n",
    "    StructField(\"precip_hrly\", DoubleType(), True),\n",
    "    StructField(\"an_prognose\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create mock data\n",
    "data = [\n",
    "    (1, \"01.01.2024 08:00\", 101, 5.0, 2.0, 15.0, 0.0, \"01.01.2024 08:05:00\"),\n",
    "    (2, \"01.01.2024 09:00\", 102, 3.0, 1.5, 18.0, 0.1, \"01.01.2024 09:10:00\"),\n",
    "    (3, \"01.01.2024 10:00\", 103, 2.0, 1.0, 20.0, 0.0, \"01.01.2024 10:02:00\"),\n",
    "    (4, \"01.01.2024 11:00\", 104, 4.0, 2.0, 22.0, 0.2, \"01.01.2024 11:15:00\"),\n",
    "    (5, \"01.01.2024 12:00\", 105, 1.0, 0.5, 25.0, 0.0, \"01.01.2024 12:00:30\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "mock_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "# Convert time str to timestamp type\n",
    "mock_df = mock_df.withColumn(\"scheduled_arrival\", F.to_timestamp(\"ankunftszeit\", \"dd.MM.yyyy HH:mm\"))\n",
    "mock_df = mock_df.withColumn(\"actual_arrival\", F.to_timestamp(\"an_prognose\", \"dd.MM.yyyy HH:mm:ss\"))\n",
    "\n",
    "# Calculate delay in minutes\n",
    "mock_df = mock_df.withColumn(\"arrival_delay\", (F.unix_timestamp(\"actual_arrival\") - F.unix_timestamp(\"scheduled_arrival\")) / 60)\n",
    "\n",
    "# Create labels for each minute of delay from 0 to 30, rounding up to the nearest integer\n",
    "mock_df = mock_df.withColumn(\"delay_label\", F.when(F.col(\"arrival_delay\") >= 30, 30).otherwise(F.ceil(F.col(\"arrival_delay\")).cast(\"int\")))\n",
    "\n",
    "# Extract hour and minute from scheduled_arrival for feature engineering\n",
    "mock_df = mock_df.withColumn(\"arrival_hour\", F.hour(\"scheduled_arrival\"))\n",
    "mock_df = mock_df.withColumn(\"arrival_minute\", F.minute(\"scheduled_arrival\"))\n",
    "\n",
    "# Define feature columns\n",
    "feature_cols = [\"bpuic\", \"linien_id\", \"avg_delay\", \"stddev_delay\", \"temperature\", \"precip_hrly\", \"arrival_hour\", \"arrival_minute\"]\n",
    "\n",
    "# Create stages for pipeline\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "classifier = RandomForestClassifier(labelCol=\"delay_label\", featuresCol=\"scaled_features\", numTrees=20)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, classifier])\n",
    "\n",
    "# Define the parameter grid with an expanded range for numTrees\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.numTrees, [20, 50, 100, 200]) \\\n",
    "    .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# Configure cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"delay_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = mock_df.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "# Select relevant columns and show predictions\n",
    "predictions.select(\"bpuic\", \"ankunftszeit\", \"linien_id\", \"avg_delay\", \"stddev_delay\", \"temperature\", \"precip_hrly\", \"arrival_delay\", \"delay_label\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c71c63-a200-4bfa-a65c-ccde238d5617",
   "metadata": {},
   "source": [
    "### Preprocess istdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83836d1-6765-4c51-bc91-b2f88906bf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess `istdaten` data\n",
    "istdaten = istdaten.withColumn(\"scheduled_arrival\", F.to_timestamp(\"ankunftszeit\", \"dd.MM.yyyy HH:mm\"))\n",
    "istdaten = istdaten.withColumn(\"actual_arrival\", F.to_timestamp(\"an_prognose\", \"dd.MM.yyyy HH:mm:ss\"))\n",
    "istdaten = istdaten.withColumn(\"arrival_delay\", (F.unix_timestamp(\"actual_arrival\") - F.unix_timestamp(\"scheduled_arrival\")) / 60)\n",
    "istdaten = istdaten.filter(F.col(\"arrival_delay\").isNotNull() & (F.col(\"arrival_delay\") <= 30))\n",
    "istdaten = istdaten.withColumn(\"delay_label\", F.when(F.col(\"arrival_delay\") >= 30, 30).otherwise(F.ceil(F.col(\"arrival_delay\")).cast(\"int\")))\n",
    "\n",
    "# Join `istdaten` with `weekday_all_info` based on relevant keys\n",
    "merged_data = istdaten.join(weekday_all_info, (istdaten.bpuic == weekday_all_info.stop_id) & (istdaten.linen_id == weekday_all_info.route_id), \"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761a89ea-89f5-4dab-a9cd-e8b6c48332b0",
   "metadata": {},
   "source": [
    "### Prediction pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d1334d-e251-4206-8b86-41ab2dc99f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "feature_cols = [\"bpuic\", \"linien_id\", \"avg_delay\", \"stddev_delay\", \"temperature\", \"precip_hrly\", \"arrival_hour\", \"arrival_minute\"]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "feature_vector = assembler.transform(merged_data)  # merged_data contains all necessary info\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "\n",
    "# Use a multi-class classifier\n",
    "classifier = RandomForestClassifier(labelCol=\"delay_label\", featuresCol=\"scaled_features\", numTrees=20)\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler, classifier])\n",
    "\n",
    "# Define the parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(classifier.numTrees, [20, 50, 100, 200]) \\\n",
    "    .addGrid(classifier.maxDepth, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# Configure cross-validation\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(labelCol=\"delay_label\", predictionCol=\"prediction\", metricName=\"accuracy\"),\n",
    "                          numFolds=3)\n",
    "\n",
    "# Split data into train and test sets\n",
    "(train_data, test_data) = merged_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "model_path = f\"/user/{username}/finalproject/model\"\n",
    "# save it \n",
    "cvModel.save(model_path)\n",
    "\n",
    "# Make predictions on test data\n",
    "predictions = cvModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b35e35-3e7a-4f19-9ea8-8f28f94c9ff6",
   "metadata": {},
   "source": [
    "###  Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91dd150-a612-4f27-90a2-ff8425509130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up evaluators\n",
    "evaluatorMulti = MulticlassClassificationEvaluator(labelCol=\"delay_label\", predictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluatorMulti.evaluate(predictions)\n",
    "print(f\"Area Under ROC: {auc:.2f}\")\n",
    "\n",
    "accuracy = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"accuracy\"})\n",
    "precision = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedPrecision\"})\n",
    "recall = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"weightedRecall\"})\n",
    "f1_score = evaluatorMulti.evaluate(predictions, {evaluatorMulti.metricName: \"f1\"})\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b28162-65ff-4866-923b-3fc0fa8e6522",
   "metadata": {},
   "source": [
    "## Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfaf63e-d698-479c-81e8-23c7a83d3960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database and switch to it\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {username} LOCATION 'user/${USER}/finalproject'\")\n",
    "spark.sql(f\"USE {username}\")\n",
    "\n",
    "databases = spark.sql(\"SHOW DATABASES\")\n",
    "#print(databases.show())\n",
    "\n",
    "# Print current database to verify\n",
    "current_db = spark.sql(\"SELECT current_database()\")\n",
    "print(current_db.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d770583e-2100-4357-84f1-94950aa7198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '404' from http://iccluster080.iccluster.epfl.ch:28998/sessions/3968 with error payload: {\"msg\":\"Session '3968' not found.\"}\n"
     ]
    }
   ],
   "source": [
    "# If we want to download a dataframe in the cluster\n",
    "# into local context:\n",
    "# %%spark -o VAR_NAME\n",
    "#print(type(current_db))\n",
    "\n",
    "#%%spark -o current_db\n",
    "\n",
    "#%%local\n",
    "#print(type(current_db))\n",
    "#current_db\n",
    "    \n",
    "# Check the dataframe object locally,\n",
    "# %%local\n",
    "# type(twitter_lang)\n",
    "# twitter_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6af948-7437-4756-ab26-4f1bba924801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
