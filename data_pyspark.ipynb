{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c39a8a-3d17-459d-a154-242edd99213d",
   "metadata": {},
   "source": [
    "***\n",
    "# Robust Journey Planning\n",
    "\n",
    "**Link to project presentation:** https://drive.google.com/file/u/2/d/1JT5YS6-EReK9E0qljRGpoRuwZnPlfxE-/view?usp=sharing&pli=1\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb95dd12-c010-4675-b7dd-718466209880",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [0. Imports](#imports)\n",
    "    * [0.1 HDFS/Hive](#hive)\n",
    "    * [0.2 Spark](#spark)\n",
    "    * [0.3 Geospatial User Defined Functions](#udf)\n",
    "* [1. Data](#data)\n",
    "    * [1.1 Timetable](#timetablegeostops)\n",
    "    * [1.2 Actual Data](#actualdata)\n",
    "    * [1.3 Geo Shapes](#geoshapes)\n",
    "    * [1.4 Weather Data](#weather)\n",
    "* [2. Data Preprocessing](#datapreprocessing)\n",
    "    * [2.1 Preprocessing Timetable & Geostops](#preprocessingtimetablegeostops)\n",
    "    * [2.2 Preprocessing Istdaten Data](#preprocessingactualdata)\n",
    "* [3. Building the Transportation Graph](#transportationgraph)\n",
    "* [4. Finding the Fastest Route](#findingfastestroute)\n",
    "* [5. Modelling Delays](#modellingdelays)\n",
    "* [6. Graphical User Interface](#gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d75e0-92f3-4242-b5ec-2cf0bbbd03ff",
   "metadata": {},
   "source": [
    "## 0. Imports <a class=\"anchor\" id=\"imports\"></a>\n",
    "In this section we import necessary packages, connect to HDFS / Hive and initialize the Spark environment we will use in the assignment. Finally, we will add support for Geospatial User Defined Funtions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8aef980-73f6-41d4-8271-2135e5ba769f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# INSERT A REGION OBJECTID\n",
    "OBJECTID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548cfefa-c411-427e-8791-39c74a17f2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cluster-based imports\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.sql import SparkSession, Row, HiveContext, Window,  functions as F\n",
    "from pyspark.sql.types import IntegerType, StringType, ArrayType, StructField, StructType\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fc97714-dc6f-4083-b476-ffeee3b6290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "# Local imports for visualization and data manipulation\n",
    "from datetime import datetime\n",
    "from pyarrow.fs import HadoopFileSystem, FileSelector\n",
    "from pyhive import hive\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcd68a-add5-4e8c-a2d2-9c0059abf4aa",
   "metadata": {},
   "source": [
    "### 0.1 HDFS/Hive <a class=\"anchor\" id=\"hive\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c25515-0961-4f96-a789-887363986e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.apache.hadoop.fs.FileSystem).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop HDFS URL: hdfs://iccluster067.iccluster.epfl.ch:8020\n",
      "Username: hogenhau\n",
      "Connected to Hive at: iccluster080.iccluster.epfl.ch:10000\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Environment variables setup\n",
    "default_db = 'com490'\n",
    "hive_server = os.environ.get('HIVE_SERVER', 'iccluster080.iccluster.epfl.ch:10000')\n",
    "hadoop_fs = os.environ.get('HADOOP_DEFAULT_FS', 'hdfs://iccluster067.iccluster.epfl.ch:8020')\n",
    "hdfs = HadoopFileSystem.from_uri(hadoop_fs)\n",
    "username = os.environ.get('USER', 'anonym')\n",
    "hive_host, hive_port = hive_server.split(':')\n",
    "\n",
    "# Connect to Hive\n",
    "conn = hive.connect(host=hive_host, port=int(hive_port), username=username)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Print connection details\n",
    "print(f\"Hadoop HDFS URL: {hadoop_fs}\")\n",
    "print(f\"Username: {username}\")\n",
    "print(f\"Connected to Hive at: {hive_host}:{hive_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abec0a4d-acdc-4383-ba71-d3a6c0ad4c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /user/hogenhau/finalproject\n"
     ]
    }
   ],
   "source": [
    "%%local \n",
    "\n",
    "# Create directory for finalproject if it doesn't already exist\n",
    "base_path = f\"/user/{username}/\"\n",
    "\n",
    "# List all files and directories in the base path\n",
    "selector = FileSelector(base_path, recursive=False)\n",
    "file_info_list = hdfs.get_file_info(selector)\n",
    "\n",
    "# Directory to check\n",
    "target_directory = \"finalproject\"\n",
    "\n",
    "# Check if the target directory exists among the listed files/directories\n",
    "directory_exists = any(info.path.rstrip('/').split('/')[-1] == target_directory for info in file_info_list)\n",
    "\n",
    "if not directory_exists:\n",
    "    # Create the directory if it does not exist\n",
    "    directory_path = f\"{base_path}{target_directory}\"\n",
    "    hdfs.create_dir(directory_path, recursive=True)\n",
    "    print(f\"Directory created: {directory_path}\")\n",
    "else:\n",
    "    print(f\"Directory already exists: {base_path}{target_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70fc4c5a-18b2-40d8-9bb7-4054d540d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database hogenhau created or already exists.\n",
      "Switched to database: hogenhau\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "# Create a new database\n",
    "query = f\"CREATE DATABASE IF NOT EXISTS {username} LOCATION '/user/{username}/finalproject'\"\n",
    "cur.execute(query)\n",
    "print(f\"Database {username} created or already exists.\")\n",
    "\n",
    "# Switch to the new database\n",
    "query = f\"USE {username}\"\n",
    "cur.execute(query)\n",
    "print(f\"Switched to database: {username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78c868f8-8f0f-4e39-8df1-b88c5433244a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('geo_shapes',),\n",
       " ('sbb_csv',),\n",
       " ('sbb_csv_one_month',),\n",
       " ('sbb_istdaten_latest_parquet',),\n",
       " ('sbb_orc',),\n",
       " ('sbb_orc_istdaten',),\n",
       " ('sbb_orc_one_day',),\n",
       " ('sbb_orc_stops',),\n",
       " ('sbb_parquet',),\n",
       " ('sbb_stop_times_region',),\n",
       " ('sbb_stop_to_stop_region',),\n",
       " ('sbb_stops_lausanne',),\n",
       " ('sbb_stops_region',),\n",
       " ('twitter',),\n",
       " ('twitter_one_day',)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "cur.execute(f\"SHOW TABLES IN {username}\")\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "219d8547-4372-4024-b635-480baccf424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "\n",
    "# Make sure to give rw access to Hive and Livy\n",
    "!hdfs dfs -setfacl -R -m user:hive:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m default:user:hive:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m user:livy:rwx /user/${USER}/finalproject\n",
    "!hdfs dfs -setfacl -R -m default:user:livy:rwx /user/${USER}/finalproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c88fca14-756d-4e6b-8bee-4c5747409e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'livy'"
     ]
    }
   ],
   "source": [
    "# Remember, when not using %%local our username is 'livy'\n",
    "local_username = os.environ.get('USER', getpass.getuser())\n",
    "local_username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "deda4f85-54b4-4100-8fcd-71d809251f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hogenhau'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%local\n",
    "username"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef3203-7a82-49de-b39c-4f5dce73d777",
   "metadata": {},
   "source": [
    "### 0.2 Spark <a class=\"anchor\" id=\"Spark\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1768d20-9052-4d20-a324-440fbc7e712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f36e0d31220>\n",
      "<class 'pyspark.sql.session.SparkSession'>"
     ]
    }
   ],
   "source": [
    "# Initializing the spark session and sending %%local {username} to Spark\n",
    "sparkSession = SparkSession.builder.appName('final-project-{0}'.format(getpass.getuser())).getOrCreate()\n",
    "sparkSession.getActiveSession()\n",
    "print(sparkSession.getActiveSession())\n",
    "print(type(sparkSession))\n",
    "sc = sparkSession.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80f9046f-6778-4aaa-9b75-cbf8e312b284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'username' as 'username' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i username -t str -n username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711bb814-d6cf-49f8-89f0-084a3d703743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "| hogenhau|sbb_istdaten_late...|      false|\n",
      "| hogenhau|             sbb_csv|      false|\n",
      "| hogenhau|             sbb_orc|      false|\n",
      "| hogenhau|         sbb_parquet|      false|\n",
      "| hogenhau|  sbb_stops_lausanne|      false|\n",
      "| hogenhau|   sbb_csv_one_month|      false|\n",
      "| hogenhau|     sbb_orc_one_day|      false|\n",
      "| hogenhau|             twitter|      false|\n",
      "| hogenhau|     twitter_one_day|      false|\n",
      "| hogenhau|    sbb_stops_region|      false|\n",
      "| hogenhau|sbb_stop_to_stop_...|      false|\n",
      "| hogenhau|sbb_stop_times_re...|      false|\n",
      "| hogenhau|       sbb_orc_stops|      false|\n",
      "| hogenhau|    sbb_orc_istdaten|      false|\n",
      "| hogenhau|          geo_shapes|      false|\n",
      "+---------+--------------------+-----------+"
     ]
    }
   ],
   "source": [
    "# Check that Spark has access to personal HDFS\n",
    "spark.sql(f\"SHOW TABLES IN {username}\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4054b714-36bc-41e2-8a6b-67fb4b90fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully passed 'hadoop_fs' as 'hadoop_fs' to Spark kernel"
     ]
    }
   ],
   "source": [
    "%%send_to_spark -i hadoop_fs -t str -n hadoop_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb18555-1747-41a9-b59c-0a8f3e69d036",
   "metadata": {},
   "source": [
    "### 0.3 Geospatial User Defined Functions <a class=\"anchor\" id=\"udf\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c9a3cc4-a636-47d5-bdcf-bbc32a25a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|function              |\n",
      "+----------------------+\n",
      "|st_asbinary           |\n",
      "|st_contains           |\n",
      "|st_distance           |\n",
      "|st_geodesiclengthwgs84|\n",
      "|st_geomfromwkb        |\n",
      "|st_linestring         |\n",
      "|st_point              |\n",
      "|st_pointfromwkb       |\n",
      "|st_setsrid            |\n",
      "+----------------------+"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "ADD JARS\n",
    "    {hadoop_fs}/data/jars/esri-geometry-api-2.2.4.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-hive-2.2.0.jar\n",
    "    {hadoop_fs}/data/jars/spatial-sdk-json-2.2.0.jar\n",
    "\"\"\")\n",
    "\n",
    "# Create or replace temporary functions\n",
    "gis_functions = [\n",
    "    \"ST_Point\", \"ST_Distance\", \"ST_SetSRID\", \"ST_GeodesicLengthWGS84\",\n",
    "    \"ST_LineString\", \"ST_AsBinary\", \"ST_PointFromWKB\", \"ST_GeomFromWKB\", \"ST_Contains\"\n",
    "]\n",
    "\n",
    "for func in gis_functions:\n",
    "    spark.sql(f\"CREATE OR REPLACE TEMPORARY FUNCTION {func} AS 'com.esri.hadoop.hive.{func}'\")\n",
    "\n",
    "# Get the list of functions\n",
    "functions_df = spark.sql(\"SHOW FUNCTIONS\")\n",
    "\n",
    "# Filter and show only functions starting with 'st_'\n",
    "functions_df.filter(F.col(\"function\").startswith(\"st_\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b4ef4f-1f72-458b-a6b0-8853b20f1ad7",
   "metadata": {},
   "source": [
    "## 1. Data <a class=\"anchor\" id=\"data\"></a>\n",
    "\n",
    "In this section, we will load and preprocess first the timetable and geostops data and finally the Istdaten data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "117417ff-2f13-4e5e-ab69-0fe760187b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote USER=livy\n",
      "local USER=hogenhau"
     ]
    }
   ],
   "source": [
    "# Just to check that everything looks normal\n",
    "print(f\"remote USER={os.getenv('USER',None)}\")\n",
    "print(f\"local USER={username}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49be96b8-82f8-46ed-a094-fb4544f14621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local USER=hogenhau\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(f\"local USER={os.getenv('USER',None)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3660f7d-4d0c-4e7c-a515-70a72d3ac70b",
   "metadata": {},
   "source": [
    "### 1.1 Timetables<a class=\"anchor\" id=\"timetables\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e580414e-ca75-46e6-ad73-8a753ccf5240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---------------+--------------+-------------+--------------+\n",
      "|stop_id|           stop_name|       stop_lat|      stop_lon|location_type|parent_station|\n",
      "+-------+--------------------+---------------+--------------+-------------+--------------+\n",
      "|1100008|Zell (Wiesental),...|47.710084270235|7.859647882747|             | Parent1100008|\n",
      "|1100009|Zell (Wiesental),...|47.713191104479|7.862908767228|             | Parent1100009|\n",
      "+-------+--------------------+---------------+--------------+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "|             trip_id|arrival_time|departure_time|    stop_id|stop_sequence|pickup_type|drop_off_type|\n",
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "|5874.TA.91-14-D-j...|    19:06:00|      19:06:00|8587057:0:I|           13|          0|            0|\n",
      "|5874.TA.91-14-D-j...|    19:08:00|      19:08:00|8592812:0:Y|           14|          0|            0|\n",
      "+--------------------+------------+--------------+-----------+-------------+-----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+----------+--------------------+-------------+---------------+------------+\n",
      "|     route_id|service_id|             trip_id|trip_headsign|trip_short_name|direction_id|\n",
      "+-------------+----------+--------------------+-------------+---------------+------------+\n",
      "|91-10-A-j24-1|  TA+p60e0|1.TA.91-10-A-j24-...|Zürich HB SZU|          12890|           0|\n",
      "|91-10-A-j24-1|  TA+p60e0|10.TA.91-10-A-j24...|Zürich HB SZU|          12888|           0|\n",
      "+-------------+----------+--------------------+-------------+---------------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+----------+------+-------+---------+--------+------+--------+------+----------+--------+\n",
      "|service_id|monday|tuesday|wednesday|thursday|friday|saturday|sunday|start_date|end_date|\n",
      "+----------+------+-------+---------+--------+------+--------+------+----------+--------+\n",
      "|        TA|  TRUE|   TRUE|     TRUE|    TRUE|  TRUE|    TRUE|  TRUE|  20231210|20241214|\n",
      "|  TA+00060| FALSE|  FALSE|    FALSE|   FALSE| FALSE|    TRUE|  TRUE|  20231210|20241214|\n",
      "+----------+------+-------+---------+--------+------+--------+------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+-------------+---------+----------------+---------------+----------+----------+\n",
      "|     route_id|agency_id|route_short_name|route_long_name|route_desc|route_type|\n",
      "+-------------+---------+----------------+---------------+----------+----------+\n",
      "|91-10-A-j24-1|       78|             S10|               |         S|       109|\n",
      "|91-10-B-j24-1|       11|             S10|               |         S|       109|\n",
      "+-------------+---------+----------------+---------------+----------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "+------------+-----------+-------------+-----------------+\n",
      "|from_stop_id| to_stop_id|transfer_type|min_transfer_time|\n",
      "+------------+-----------+-------------+-----------------+\n",
      "|     1100079|    8014441|            2|              240|\n",
      "|     1100079|8014441:0:2|            2|              240|\n",
      "+------------+-----------+-------------+-----------------+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "# Load timetables data from HDFS\n",
    "stops = spark.read.orc('/data/sbb/orc/timetables/stops/year=2024/month=5/day=16')\n",
    "stop_times = spark.read.orc('/data/sbb/orc/timetables/stop_times/year=2024/month=5/day=16')\n",
    "trips = spark.read.orc('/data/sbb/orc/timetables/trips/year=2024/month=5/day=16')\n",
    "calendar = spark.read.orc('/data/sbb/orc/timetables/calendar/year=2024/month=5/day=16')\n",
    "routes = spark.read.orc('/data/sbb/orc/timetables/routes/year=2024/month=5/day=16')\n",
    "transfers = spark.read.orc('/data/sbb/orc/timetables/transfers/year=2024/month=5/day=16')\n",
    "\n",
    "stops.show(2)\n",
    "stop_times.show(2)\n",
    "trips.show(2)\n",
    "calendar.show(2)\n",
    "routes.show(2)\n",
    "transfers.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d1cb7b-60b4-44a7-b5db-e4e634d27e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------------+-----------------+\n",
      "|from_stop_id| to_stop_id|transfer_type|min_transfer_time|\n",
      "+------------+-----------+-------------+-----------------+\n",
      "|     1100079|    8014441|            2|              240|\n",
      "|     1100079|8014441:0:2|            2|              240|\n",
      "|     1100079|8014441:0:3|            2|              240|\n",
      "|     1100079|8014441:0:1|            2|              240|\n",
      "|     1100084|    8014440|            2|              180|\n",
      "|     1100084|8014440:0:1|            2|              180|\n",
      "|     1100097|    8014447|            2|              240|\n",
      "|     1100097|8014447:0:2|            2|              240|\n",
      "|     1100097|8014447:0:1|            2|              240|\n",
      "|     1100097|8014447:0:3|            2|              240|\n",
      "|     1100102|    8014446|            2|              240|\n",
      "|     1100102|8014446:0:1|            2|              240|\n",
      "|     1100158|    8014445|            2|              180|\n",
      "|     1100158|8014445:0:1|            2|              180|\n",
      "|     1100158|8014445:0:2|            2|              180|\n",
      "|     1100201|    8021703|            2|              240|\n",
      "|     1100201|8021703:0:1|            2|              240|\n",
      "|     1100303|    8030446|            2|              180|\n",
      "|     1100303|8030446:0:1|            2|              180|\n",
      "|     1100313|    8014331|            2|              180|\n",
      "+------------+-----------+-------------+-----------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "transfers.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa5293-9960-4ca6-8869-070f54eae78e",
   "metadata": {},
   "source": [
    "### 1.2 Istdaten <a class=\"anchor\" id=\"istdaten\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d70e7b4b-60db-4c67-902d-a15224a807b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+----+-----+\n",
      "|betriebstag|fahrt_bezeichner|betreiber_id|betreiber_abk|betreiber_name|produkt_id|linien_id|linien_text|umlauf_id|verkehrsmittel_text|zusatzfahrt_tf|faellt_aus_tf|  bpuic|   haltestellen_name|    ankunftszeit|        an_prognose|an_prognose_status|    abfahrtszeit|        ab_prognose|ab_prognose_status|durchfahrt_tf|year|month|\n",
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+----+-----+\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506101|   Felben-Wellhausen|22.12.2023 09:36|22.12.2023 09:36:34|              REAL|22.12.2023 09:36|22.12.2023 09:36:53|              REAL|        false|2023|   12|\n",
      "| 22.12.2023|  85:65:8031:001|       85:65|       THURBO|        THURBO|       Zug|     8031|        S30|         |                  S|         false|        false|8506102|Hüttlingen-Metten...|22.12.2023 09:39|22.12.2023 09:39:08|              REAL|22.12.2023 09:39|22.12.2023 09:39:26|              REAL|        false|2023|   12|\n",
      "+-----------+----------------+------------+-------------+--------------+----------+---------+-----------+---------+-------------------+--------------+-------------+-------+--------------------+----------------+-------------------+------------------+----------------+-------------------+------------------+-------------+----+-----+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "istdaten = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "\n",
    "#istdaten.printSchema()\n",
    "istdaten.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c751a612-4a17-4f29-b17d-16c238cd7c35",
   "metadata": {},
   "source": [
    "### 1.3 Geo Shapes <a class=\"anchor\" id=\"geoshapes\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e4dabf-5fac-499a-8acb-71271c549722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[]"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {username}.geo_shapes\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE EXTERNAL TABLE {username}.geo_shapes(\n",
    "    objectid INT,\n",
    "    name     STRING,\n",
    "    geometry BINARY\n",
    ")\n",
    "PARTITIONED BY(country STRING, region STRING)\n",
    "ROW FORMAT SERDE 'com.esri.hadoop.hive.serde.GeoJsonSerDe' \n",
    "STORED AS INPUTFORMAT 'com.esri.json.hadoop.UnenclosedEsriJsonInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION '/data/geo/json/'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0adc38f-34ef-489e-9fc5-3044b8d9c82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+-------+------+\n",
      "|objectid|          name|            geometry|country|region|\n",
      "+--------+--------------+--------------------+-------+------+\n",
      "|     109|           Uri|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1016|    Schattdorf|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1124|  Altdorf (UR)|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1177|      Isenthal|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1295|  Seedorf (UR)|[00 00 10 E6 06 1...|     CH|    UR|\n",
      "|    1344|     Andermatt|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1460|UnterschÃ¤chen|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1592|  Attinghausen|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1646|    GÃ¶schenen|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "|    1686|     Hospental|[00 00 10 E6 03 1...|     CH|    UR|\n",
      "+--------+--------------+--------------------+-------+------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"MSCK REPAIR TABLE {username}.geo_shapes\")\n",
    "geo_shapes = spark.sql(f\"SELECT * FROM {username}.geo_shapes\")\n",
    "geo_shapes.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d47933-ba27-4799-97f6-dd9545795be7",
   "metadata": {},
   "source": [
    "### 1.4 Optional: Weather Data <a class=\"anchor\" id=\"weatherdata\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58415e-7ac7-42cd-861e-9ca30eaa0570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33a0083b-23cb-43ff-a480-ccddb793fef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add code from hw3\n",
    "#stations = spark.read.csv('/data/wunderground/csv/stations', header=True)\n",
    "#weather_data = spark.read.json('/data/wunderground/json/history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca34514-08bb-4b2b-8a1a-2189d616855c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "799d8615-f567-40bc-801a-7b09400a2d10",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing <a class=\"anchor\" id=\"datapreprocessing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1351c1e-99f6-434c-81d9-3a8fc192dce7",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Timetable & Geostops <a class=\"anchor\" id=\"preprocessingtimetablegeostops\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7d6284b-8849-432b-8896-ec49d02f52ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+---------------+--------------+--------------+\n",
      "|    stop_id|           stop_name|       stop_lat|      stop_lon|parent_station|\n",
      "+-----------+--------------------+---------------+--------------+--------------+\n",
      "|    8501075|Lausanne-Ouchy (lac)|46.505077550443|6.627608470405| Parent8501075|\n",
      "|8501117:0:1|            Bussigny|46.547260262317|6.552428464267| Parent8501117|\n",
      "|8501117:0:3|            Bussigny|46.546957528818|6.552850672450| Parent8501117|\n",
      "|8501117:0:4|            Bussigny|46.546914281038|6.552805756686| Parent8501117|\n",
      "|8501118:0:1|           Renens VD|46.536206306037|6.580923025083| Parent8501118|\n",
      "+-----------+--------------------+---------------+--------------+--------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "stops.createOrReplaceTempView(\"stops\")\n",
    "\n",
    "stops_region = spark.sql(f\"\"\"\n",
    "SELECT\n",
    "    a.stop_id,\n",
    "    a.stop_name,\n",
    "    a.stop_lat,\n",
    "    a.stop_lon,\n",
    "    a.parent_station\n",
    "FROM stops a JOIN {username}.geo_shapes b\n",
    "ON ST_Contains(b.geometry, ST_Point(a.stop_lon, a.stop_lat))\n",
    "WHERE b.objectid = {OBJECTID}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "stops_region.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bec2387f-81de-4b77-a689-a22dac209ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips full week: 38694\n",
      "+----------+\n",
      "|service_id|\n",
      "+----------+\n",
      "|  TA+24k40|\n",
      "|  TA+2s440|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Trips Only active on Weekends: 3670"
     ]
    }
   ],
   "source": [
    "# We choose to focus only on the weekday schedule\n",
    "print(f\"Trips full week: {calendar.count()}\")\n",
    "\n",
    "# Filter for services that are active from Monday to Friday\n",
    "# We want to filter out services that are only active in the weekends\n",
    "\n",
    "weekend_calendar = calendar.filter(\n",
    "    (F.col(\"monday\") == False) &\n",
    "    (F.col(\"tuesday\") == False) &\n",
    "    (F.col(\"wednesday\") == False) &\n",
    "    (F.col(\"thursday\") == False) &\n",
    "    (F.col(\"friday\") == False) &\n",
    "    (F.col(\"saturday\") == True) &\n",
    "    (F.col(\"sunday\") == True)\n",
    ").select(\"service_id\").distinct()\n",
    "\n",
    "weekend_calendar.show(2)\n",
    "\n",
    "print(f\"Trips Only active on Weekends: {weekend_calendar.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21e35929-6fd2-4d20-87ad-60a04e23146c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct trip_ids before filtering: 60415\n",
      "Distinct trip_ids after filtering: 44013\n",
      "+--------------------+------------+--------------+-----------+-------------+-------------+---------+---------------+--------------+--------------+----------+\n",
      "|             trip_id|arrival_time|departure_time|    stop_id|stop_sequence|     route_id|stop_name|       stop_lat|      stop_lon|parent_station|route_desc|\n",
      "+--------------------+------------+--------------+-----------+-------------+-------------+---------+---------------+--------------+--------------+----------+\n",
      "|1.TA.91-1R-Y-j24-...|    18:57:00|      19:01:00|8501120:0:1|            2|91-1R-Y-j24-1| Lausanne|46.516774559699|6.629512898808| Parent8501120|       EXT|\n",
      "|1.TA.91-2E-Y-j24-...|    19:45:00|      19:45:00|8501120:0:6|            1|91-2E-Y-j24-1| Lausanne|46.516521109968|6.629018825402| Parent8501120|       TGV|\n",
      "|1.TA.91-2H-Y-j24-...|    13:43:00|      13:43:00|8501120:0:4|            1|91-2H-Y-j24-1| Lausanne|46.516669470930|6.629054758013| Parent8501120|        IC|\n",
      "|1.TA.91-37-Y-j24-...|    18:23:00|      18:23:00|8501120:0:7|            1|91-37-Y-j24-1| Lausanne|46.516459292781|6.629000859096| Parent8501120|       TGV|\n",
      "|1.TA.91-3D-Y-j24-...|    08:08:00|      08:14:00|8501118:0:4|            3|91-3D-Y-j24-1|Renens VD|46.536354613245|6.581066755529| Parent8501118|        IR|\n",
      "+--------------------+------------+--------------+-----------+-------------+-------------+---------+---------------+--------------+--------------+----------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Connections table\n",
    "# Register the DataFrames as temporary views\n",
    "stops_region.createOrReplaceTempView(\"stops_region\")\n",
    "stops.createOrReplaceTempView(\"stops\")\n",
    "stop_times.createOrReplaceTempView(\"stop_times\")\n",
    "trips.createOrReplaceTempView(\"trips\")\n",
    "routes.createOrReplaceTempView(\"routes\")\n",
    "calendar.createOrReplaceTempView(\"calendar\")\n",
    "\n",
    "\n",
    "# Perform an anti join to filter out weekend trips\n",
    "weekday_trips = trips.join(\n",
    "    weekend_calendar, \n",
    "    trips.service_id == weekend_calendar.service_id, \n",
    "    \"left_anti\"\n",
    ")\n",
    "\n",
    "# Give alias because of ambiguous \"trip_id\"\n",
    "stop_times_joined = stop_times.alias(\"st\").join(\n",
    "    weekday_trips.alias(\"wt\"),\n",
    "    F.col(\"st.trip_id\") == F.col(\"wt.trip_id\"),\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "\n",
    "final_df = stop_times_joined.join(\n",
    "    stops_region.alias(\"sr\"),\n",
    "    F.col(\"st.stop_id\") == F.col(\"sr.stop_id\"),\n",
    "    \"inner\"\n",
    ").select(\n",
    "    F.col(\"st.trip_id\").alias(\"trip_id\"),\n",
    "    F.col(\"st.arrival_time\").alias(\"arrival_time\"),\n",
    "    F.col(\"st.departure_time\").alias(\"departure_time\"),\n",
    "    F.col(\"st.stop_id\").alias(\"stop_id\"),\n",
    "    F.col(\"st.stop_sequence\").alias(\"stop_sequence\"),\n",
    "    F.col(\"wt.route_id\").alias(\"route_id\"),\n",
    "    F.col(\"sr.stop_name\").alias(\"stop_name\"),\n",
    "    F.col(\"sr.stop_lat\").alias(\"stop_lat\"),\n",
    "    F.col(\"sr.stop_lon\").alias(\"stop_lon\"),\n",
    "    F.col(\"sr.parent_station\").alias(\"parent_station\")\n",
    ").orderBy(\"trip_id\", \"stop_sequence\")\n",
    "\n",
    "# Add ROUTES_DESC from the routes table to include transport type\n",
    "final_df = final_df.join(\n",
    "    routes,\n",
    "    final_df.route_id == routes.route_id,\n",
    "    \"left\"  # Left join to keep all records from 'final_df' even if no match in 'routes'\n",
    ").select(\n",
    "    final_df[\"*\"],  # Keep all existing columns from 'final_df'\n",
    "    routes.route_desc  # Add the 'ROUTE_DESC' column from 'routes'\n",
    ")\n",
    "\n",
    "\n",
    "# Create a filter to select only the times between 06:00:00 and 20:00:00\n",
    "time_filter = (F.col(\"departure_time\") >= \"06:00:00\") & (F.col(\"departure_time\") <= \"20:00:00\") & (F.col(\"arrival_time\") >= \"06:00:00\") & (F.col(\"arrival_time\") >= \"06:00:00\")\n",
    "\n",
    "print(f\"Distinct trip_ids before filtering: {final_df.select('trip_id').distinct().count()}\")\n",
    "final_df = final_df.filter(time_filter)\n",
    "print(f\"Distinct trip_ids after filtering: {final_df.select('trip_id').distinct().count()}\")\n",
    "final_df.show(5)\n",
    "\n",
    "# Print headers to copy and load them manually in local pd.DataFrame\n",
    "#final_df_headers = final_df.columns\n",
    "#print(final_df_headers)\n",
    "\n",
    "# Write PySpark dataframe to csv\n",
    "#final_df.write.csv(f\"/user/{username}/folder/final_df\", header=False, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83a0a0ad-8767-4a26-8932-90b14b6076aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%local\n",
    "#!hdfs dfs -getmerge /user/{username}/folder/final_df final_df_folder/final_df_combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3b8a4be-b262-4a45-893d-36f06d19559b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cb22a994ce42c1bb6a6cf60c390922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28577b1dc44f427dab1e0a01c6bacf32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local\n",
    "final_df = pd.read_csv(\"final_df_folder/final_df_combined.csv\", header=None)\n",
    "final_df.columns = ['trip_id', 'arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'route_id', 'stop_name', 'stop_lat', 'stop_lon', 'parent_station', 'route_desc']\n",
    "final_df = final_df.sort_values(by=['trip_id', 'stop_sequence'])\n",
    "\n",
    "# Copy 'stop_id' to 'arrival_stop_id'\n",
    "final_df['arrival_stop_id'] = final_df['stop_id']\n",
    "\n",
    "# Create 'departure_stop_id' by shifting 'stop_id' within each group\n",
    "final_df['departure_stop_id'] = final_df.groupby('trip_id')['stop_id'].shift(-1)\n",
    "\n",
    "# Helper function to convert HH:MM:SS time string to seconds past midnight\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    return h * 3600 + m * 60 + s\n",
    "\n",
    "# Apply the time conversion to both arrival and departure times\n",
    "final_df['arrival_time_seconds'] = final_df['arrival_time'].apply(time_to_seconds)\n",
    "final_df['departure_time_seconds'] = final_df['departure_time'].apply(time_to_seconds)\n",
    "\n",
    "# Calculate 'travel_time_seconds' as the difference between the 'arrival_time_seconds' of the next stop\n",
    "# and 'departure_time_seconds' of the current stop, within each trip\n",
    "final_df['travel_time_seconds'] = final_df['arrival_time_seconds'] - final_df.groupby('trip_id')['departure_time_seconds'].shift(1)\n",
    "\n",
    "# Now you can check your dataframe\n",
    "final_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfecf66d-ad10-4cbf-9bb3-d7aff27e769c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97e9158f9974eaa82b67d1a2799c3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(HTML(value='Type:'), Button(description='Table', layout=Layout(width='70px'), st…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd369aaaf9a469e8467ae82a101693d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local \n",
    "df = pd.read_pickle(\"./final_df.pkl\")\n",
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff098eb-906c-4faa-af5b-2338f006a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44013\n",
      "89\n",
      "544\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(len(final_df.trip_id.unique()))\n",
    "print(len(final_df.route_id.unique()))\n",
    "print(len(final_df.stop_id.unique()))\n",
    "final_df.to_pickle(\"./final_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed2bd58e-9b6d-4550-8936-cc74e99d5c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+--------------------+---------------+--------------+---------------+--------------+---------------+---------------+-----------------+---------------------+\n",
      "|stop_id1|stop_id2|          stop_name1|          stop_name2|      stop_lat1|     stop_lon1|      stop_lat2|     stop_lon2|parent_station1|parent_station2|  distance_meters|transfer_time_minutes|\n",
      "+--------+--------+--------------------+--------------------+---------------+--------------+---------------+--------------+---------------+---------------+-----------------+---------------------+\n",
      "| 8501075| 8591986|Lausanne-Ouchy (lac)|Lausanne, Beau-Ri...|46.505077550443|6.627608470405|46.508304989443|6.627500672571|  Parent8501075|  Parent8591986|358.8614310087467|    9.177228620174933|\n",
      "| 8501075| 8592086|Lausanne-Ouchy (lac)|Lausanne, Ouchy-O...|46.505077550443|6.627608470405|46.507470325313|6.626638289898|  Parent8501075|  Parent8592086|276.2107661651952|   7.5242153233039035|\n",
      "+--------+--------+--------------------+--------------------+---------------+--------------+---------------+--------------+---------------+---------------+-----------------+---------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "['stop_id1', 'stop_id2', 'stop_name1', 'stop_name2', 'stop_lat1', 'stop_lon1', 'stop_lat2', 'stop_lon2', 'parent_station1', 'parent_station2', 'distance_meters', 'transfer_time_minutes']"
     ]
    }
   ],
   "source": [
    "stop_pairs_within_500m = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    a.stop_id AS stop_id1,\n",
    "    b.stop_id AS stop_id2,\n",
    "    a.stop_name AS stop_name1,\n",
    "    b.stop_name AS stop_name2,\n",
    "    a.stop_lat AS stop_lat1,\n",
    "    a.stop_lon AS stop_lon1,\n",
    "    b.stop_lat AS stop_lat2,\n",
    "    b.stop_lon AS stop_lon2,\n",
    "    a.parent_station AS parent_station1,\n",
    "    b.parent_station AS parent_station2,\n",
    "    ST_GeodesicLengthWGS84(ST_SetSRID(ST_LineString(a.stop_lon, a.stop_lat, b.stop_lon, b.stop_lat), 4326)) AS distance_meters\n",
    "FROM \n",
    "    stops_region a \n",
    "CROSS JOIN \n",
    "    stops_region b\n",
    "WHERE \n",
    "    a.stop_id != b.stop_id\n",
    "    AND ST_GeodesicLengthWGS84(ST_SetSRID(ST_LineString(array(ST_Point(a.stop_lon, a.stop_lat), ST_Point(b.stop_lon, b.stop_lat))), 4326)) < 500\n",
    "\"\"\")\n",
    "\n",
    "# Calculate walking transfer times in minutes\n",
    "# We assume that 2min mininum are required for transfers within a same location, \n",
    "# to which we add 1min per 50m walking time to connect two stops at most 500m appart\n",
    "stop_pairs_within_500m = stop_pairs_within_500m.withColumn(\n",
    "    \"transfer_time_minutes\",\n",
    "    F.expr(\"2 + (distance_meters / 50)\")\n",
    ")\n",
    "# Show results\n",
    "stop_pairs_within_500m.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0480b-7ca6-4357-be93-9a092aab5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print headers to copy and load them manually in local pd.DataFrame\n",
    "#stop_pairs_within_500m_headers = stop_pairs_within_500m.columns\n",
    "#print(stop_pairs_within_500m_headers)\n",
    "\n",
    "# Write PySpark dataframe to csv\n",
    "#stop_pairs_within_500m.write.csv(f\"/user/{username}/folder/stop_pairs_within_500m_folder\", header=False, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17f45f6a-c7a2-4492-a518-b97af46eaad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%local\n",
    "#!hdfs dfs -getmerge /user/{username}/folder/stop_pairs_within_500m_folder stop_pairs_within_500m_folder/stop_pairs_within_500m_df_combined.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b689d1ad-0e29-4b02-925f-405fd961dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "stop_pairs_within_500m = pd.read_csv(\"stop_pairs_within_500m_folder/stop_pairs_within_500m_df_combined.csv\", header=None)\n",
    "stop_pairs_within_500m.columns = ['stop_id1', 'stop_id2', 'stop_name1', 'stop_name2', 'stop_lat1', 'stop_lon1', 'stop_lat2', 'stop_lon2', 'parent_station1', 'parent_station2', 'distance_meters', 'transfer_time_minutes']\n",
    "\n",
    "def minutes_to_seconds(minutes):\n",
    "    return minutes * 60\n",
    "\n",
    "stop_pairs_within_500m['transfer_time_seconds'] = stop_pairs_within_500m['transfer_time_minutes'].apply(minutes_to_seconds)\n",
    "stop_pairs_within_500m.head()\n",
    "#stop_pairs_within_500m.to_pickle(\"./stop_pairs_within_500m.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1bf3f9-62ba-4047-b65d-a551027ec6f4",
   "metadata": {},
   "source": [
    "## 3. Building the Transportation Graph <a class=\"anchor\" id=\"transportationgraph\"></a>\n",
    "- NetworkX doesn't work in the PySpark kernel. So we need to run it in `%%local`.\n",
    "    - For that we need all our PySpark dataframes loaded locally.\n",
    "    - **Instead, refer to our python kernel jupyter notebook**: \"graph_build_and_search.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48747163-7e7d-4d60-a956-e359e4cba95e",
   "metadata": {},
   "source": [
    "## 4. Finding the Fastest Route <a class=\"anchor\" id=\"findingfastestroute\"></a>\n",
    "**Refer to our python kernel jupyter notebook**: \"graph_build_and_search.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42680931-a19b-4713-87d8-6f156ea424a9",
   "metadata": {},
   "source": [
    "## 5. Modelling Delays <a class=\"anchor\" id=\"modellingdelays\"></a>\n",
    "**Refer to our python kernel jupyter notebooks and .py scripts**: \n",
    "- \"data_pyspark_model.ipynb\"\n",
    "- \"model_training_and_inference_sklearn.ipynb\"\n",
    "- \"ml.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb8474-e9b2-45df-bbe6-76605a10257a",
   "metadata": {},
   "source": [
    "## 6. Graphical User Interface <a class=\"anchor\" id=\"gui\"></a>\n",
    "**Refer to:** interface.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
